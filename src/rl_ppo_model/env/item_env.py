from .config.utils import load_scene_config

class EnvClass:
    def __init__(self):
        self.scene = None
        self.objects = []
        self.agent = None  # å¯å¾ŒçºŒæ¥å…¥ PPO agent

    def load_scene(self, scene_data: dict):
        self.scene = scene_data
        self.objects = scene_data.get("objects", [])

    def step_from_state(self, state: dict):
        obs = self._convert_state_to_obs(state)
        action = self.agent.predict(obs) if self.agent else [0, 0, 0]
        reward = self._compute_reward(state, action)
        return action, reward

    def _convert_state_to_obs(self, state):
        return [obj["position"] for obj in state.get("objects", [])]

    def _compute_reward(self, state, action):
        return 1.0  # æš«æ™‚å›ºå®šå›é¥‹å€¼


if __name__ == "__main__":
    # ğŸ§ª å¯ä½œç‚ºç’°å¢ƒæ¸¬è©¦ç”¨çš„å°å·¥å…·
    scene = load_scene_config()
    print("[åˆå§‹åŒ–å®¹å™¨]", scene["environment_meta"]["container"])
    print("[ç‰©ä»¶æ•¸é‡]", len(scene["objects"]))

    env = EnvClass()
    env.load_scene(scene)

    # æ¨¡æ“¬åŸ·è¡Œä¸€æ­¥è¡Œå‹•
    dummy_state = scene  # æ¸¬è©¦ç”¨ï¼ˆé€šå¸¸ä¾†è‡ªå‰ç«¯ï¼‰
    action, reward = env.step_from_state(dummy_state)
    print("[Agent å‹•ä½œ]", action)
    print("[Reward]", reward)