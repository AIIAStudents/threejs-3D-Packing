import os
import time
from stable_baselines3 import PPO
from rl_ppo_model.env.custom_env import CustomEnv

# ğŸ“ é è¨­æ¨¡å‹å„²å­˜è·¯å¾‘
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
default_model_dir = os.path.join(base_dir, 'models', 'default')
default_model_path = os.path.join(default_model_dir, 'default_model.zip')
os.makedirs(default_model_dir, exist_ok=True)

def train_model(env, total_steps=100_000, save_path=default_model_path):
    """è¨“ç·´ PPO æ¨¡å‹ä¸¦å„²å­˜è‡³æŒ‡å®šè·¯å¾‘"""
    print(f"ğŸš€ é–‹å§‹è¨“ç·´é è¨­æ¨¡å‹ï¼Œå„²å­˜è‡³ï¼š{save_path}")
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=total_steps)
    model.save(save_path)
    print(f"âœ… é è¨­æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜è‡³ {save_path}")

def run_training_step(env, model_path=None):
    """
    å–®æ­¥æ¨è«–ï¼šè‹¥æ¨¡å‹ä¸å­˜åœ¨å‰‡ä½¿ç”¨é è¨­æ¨¡å‹è¨“ç·´
    - envï¼šå·²æº–å‚™å¥½çš„ CustomEnv
    - model_pathï¼šç›®æ¨™æ¨¡å‹è·¯å¾‘ï¼ˆå¯é¸ï¼‰
    """
    # å¦‚æœæ²’æŒ‡å®šæ¨¡å‹è·¯å¾‘ï¼Œæˆ–æŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°±ä½¿ç”¨é è¨­æ¨¡å‹
    if not model_path or not os.path.exists(model_path):
        print("âš ï¸ æ¨¡å‹è·¯å¾‘ç„¡æ•ˆæˆ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨é è¨­æ¨¡å‹é‡æ–°è¨“ç·´")
        train_model(env)  # ä½¿ç”¨é è¨­è·¯å¾‘è¨“ç·´
        model_path = default_model_path

    # è¼‰å…¥æ¨¡å‹ä¸¦åŸ·è¡Œæ¨è«–
    model = PPO.load(model_path, env=env)
    obs, info = env.reset()
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    return action.tolist(), reward

if __name__ == "__main__":
    # åˆå§‹åŒ–ç’°å¢ƒï¼ˆè«‹æ›¿æ›æˆä½ çš„çœŸå¯¦å ´æ™¯è³‡æ–™ï¼‰
    scene_data = {"objects": []}
    env = CustomEnv(scene_data)

    # å˜—è©¦æ¨è«–ï¼ˆå¯æŒ‡å®šæ¨¡å‹è·¯å¾‘ï¼‰
    action, reward = run_training_step(env, model_path=None)
    print(f"ğŸ¯ æ¨è«–çµæœï¼šaction={action}, reward={reward}")