import os
from stable_baselines3 import PPO
from rl_ppo_model.env.custom_env import CustomEnv

def train_model(env, total_steps=100_000, save_path="ppo_model.zip"):
    """è¨“ç·´ PPO æ¨¡å‹ä¸¦å„²å­˜"""
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=total_steps)
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… æ¨¡å‹å„²å­˜è‡³ {save_path}")

def run_training_step(env, model_path=None, auto_train_if_missing=True):
    """
    å–®æ­¥æ¨è«–ï¼šè‡ªå‹•ä½¿ç”¨é è¨­æ¨¡å‹ï¼Œä¸¦åœ¨ç¼ºå¤±æ™‚è‡ªå‹•è¨“ç·´
    - envï¼šå·²åˆå§‹åŒ–çš„ CustomEnv
    - model_pathï¼šå¯é¸ï¼Œè‹¥æœªæä¾›å‰‡ä½¿ç”¨é è¨­æ¨¡å‹
    - auto_train_if_missingï¼šè‹¥æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ˜¯å¦è‡ªå‹•è¨“ç·´
    """
    # é è¨­æ¨¡å‹è·¯å¾‘
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    default_model_path = os.path.join(base_dir, 'models', 'default', 'default_model.zip')

    # ä½¿ç”¨é è¨­æ¨¡å‹è·¯å¾‘ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if model_path is None:
        print("ğŸ“¢ æœªæŒ‡å®šæ¨¡å‹è·¯å¾‘ï¼Œä½¿ç”¨é è¨­æ¨¡å‹")
        model_path = default_model_path

    # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ ¹æ“šè¨­å®šè‡ªå‹•è¨“ç·´
    if not os.path.exists(model_path):
        if auto_train_if_missing:
            print(f"âš ï¸ æ¨¡å‹ä¸å­˜åœ¨ï¼š{model_path}\nğŸš€ é–‹å§‹è‡ªå‹•è¨“ç·´é è¨­æ¨¡å‹...")
            train_model(env, total_steps=100_000, save_path=model_path)
        else:
            raise FileNotFoundError(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼š{model_path}")

    # è¼‰å…¥æ¨¡å‹ä¸¦æ¨è«–
    model = PPO.load(model_path, env=env)
    obs, info = env.reset()
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    return action.tolist(), reward

if __name__ == "__main__":
    scene_data = {"objects": []}  # â† æ›æˆä½ çš„çœŸå¯¦å ´æ™¯è³‡æ–™
    env = CustomEnv(scene_data)

    # ä¸æŒ‡å®šæ¨¡å‹ â†’ è‡ªå‹•ä½¿ç”¨é è¨­æ¨¡å‹ï¼Œä¸¦åœ¨ç¼ºå¤±æ™‚è‡ªå‹•è¨“ç·´
    action, reward = run_training_step(env)
    print(f"ğŸ¯ æ¨è«–çµæœï¼šaction={action}, reward={reward}")
    
