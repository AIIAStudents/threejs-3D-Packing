import os
import numpy as np
from stable_baselines3 import PPO
from env.custom_env import CustomEnv

def evaluate_model(model_path, num_episodes=10, render=False):
    # å»ºç«‹ç’°å¢ƒ
    env = CustomEnv()

    # è¼‰å…¥è¨“ç·´å®Œæˆçš„æ¨¡å‹
    model = PPO.load(model_path)

    total_rewards = []
    total_successes = 0
    total_actions = 0

    print(f"ğŸš€ é–‹å§‹æ¨¡å‹æ¸¬è©¦ï¼Œå…±åŸ·è¡Œ {num_episodes} å€‹å ´æ™¯")

    for episode in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        episode_success = 0

        while not done:
            action, _states = model.predict(obs)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated  # åˆä½µæˆèˆŠç‰ˆçš„ done åˆ¤æ–·
            episode_reward += reward
            total_actions += 1
            if info.get("status") == "success":
                episode_success += 1
            if render:
                env.render()

        total_rewards.append(episode_reward)
        total_successes += episode_success

        print(f"[Episode {episode+1}] Reward = {episode_reward:.2f}, Successes = {episode_success}")

    avg_reward = np.mean(total_rewards)
    avg_success_rate = total_successes / total_actions if total_actions > 0 else 0

    print("\nğŸ“Š æ¨¡å‹è©•ä¼°çµæœ:")
    print(f"ğŸ”¹ å¹³å‡ Reward: {avg_reward:.2f}")
    print(f"ğŸ”¹ å¹³å‡æˆåŠŸç‡: {avg_success_rate*100:.2f}%")

if __name__ == "__main__":
    model_path = "models/ppo_cube_v1.pt"
    evaluate_model(model_path, num_episodes=10, render=False)